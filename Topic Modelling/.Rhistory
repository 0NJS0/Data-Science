library(rvest)
library(tm)
library(tokenizers)
library(hunspell)
library(textstem)
library(textclean)
url <- "https://en.wikipedia.org/wiki/American_International_University-Bangladesh"
webpage <- read_html(url)
text_data <- html_text(html_nodes(webpage, "p"))
lower_case_data <- tolower(text_data)
print(lower_case_data)
no_punc_data <- gsub("[[:punct:]]", "", lower_case_data)
print(no_punc_data)
no_digit_data <- gsub("\\d+", "", no_punc_data)
no_space_data <- stripWhitespace(no_digit_data)
no_contraction_text <- replace_contraction(no_space_data)
emoji_pattern <- "[\U0001F600-\U0001F64F]"
emoticon_pattern <- "[:;]-?[)D(|P]"
text_no_emojis <- sapply(no_contraction_text, function(text) {
text <- gsub(emoji_pattern, "", text, perl = TRUE)
text <- gsub(emoticon_pattern, "", text, perl = TRUE)
return(text)
})
text_no_stopwords <- removeWords(text_no_emojis, stopwords("en"))
remove_misspelled_words <- function(text) {
words <- unlist(tokenize_words(text))
is_correct <- hunspell_check(words)
corrected_words <- words[is_correct]
return(paste(corrected_words, collapse = " "))
}
spell_checked_text <- sapply(text_no_stopwords, remove_misspelled_words)
tokens <- tokenize_words(spell_checked_text)
clean_tokens <- unlist(tokens)
clean_tokens <- lemmatize_words(clean_tokens)
clean_tokens <- clean_tokens[nchar(clean_tokens) > 1]
tokens_df <- data.frame(tokens = clean_tokens, stringsAsFactors = FALSE)
View(tokens_df)
write.csv(tokens_df, "D://Educational/AIUB sem 9/MID/Data Science/Project2.csv", row.names = FALSE)
library(tm)
library(topicmodels)
corpus <- Corpus(VectorSource(tokens_df$tokens))
dtm <- DocumentTermMatrix(corpus)
row_sums <- rowSums(as.matrix(dtm))
print(row_sums)
dtm <- dtm[row_sums > 0,99 ]
k=3
lda_model <- LDA(dtm, k = k, control = list(seed = 123))
# View top terms
print(terms(lda_model,4))
library(tm)
library(topicmodels)
corpus <- Corpus(VectorSource(tokens_df$tokens))
dtm <- DocumentTermMatrix(corpus)
row_sums <- rowSums(as.matrix(dtm))
print(row_sums)
dtm <- dtm[row_sums > 0,99 ]
k=3
lda_model <- LDA(dtm, k = k, control = list(seed = 123))
lda_model <- LDA(dtm, k = 3, control = list(seed = 123))
file_path <- "D://Educational/AIUB sem 9/MID/Data Science/Project2.csv"
data <- read.csv(file_path, stringsAsFactors = FALSE)
corpus <- Corpus(VectorSource(data$tokens))
corpus <- tm_map(corpus, content_transformer(tolower))      # Convert to lowercase
corpus <- Corpus(VectorSource(data$tokens))
dtm <- DocumentTermMatrix(corpus)
tfidf <- weightTfIdf(dtm)
print(tfidf)
k <- 5
lda_model <- LDA(tfidf, k = k, control = list(seed = 123))
lda_model <- LDA(dtm, k = k, control = list(seed = 123))
row_totals <- apply(dtm, 1, sum)
dtm <- dtm[row_totals > 0, ]
k <- 5
lda_model <- LDA(dtm, k = k, control = list(seed = 123))
topics <- terms(lda_model, 10)
print(topics)
library(LDAvis)
install.packages("LDavis")
topic_term_weights <- posterior(lda_model)$terms
print("Topic-Term Weights:")
for (i in 1:k) {
cat(sprintf("Topic %d:\n", i))
token_weights <- sort(topic_term_weights[i, ], decreasing = TRUE)  # Sort by weight
print(head(token_weights, 10))  # Display top 10 tokens with their weights
cat("\n")
}
topics <- terms(lda_model, 10)
print(topics)
library(LDAvis)
lda_json <- createJSON(
phi = posterior(lda_model)$terms,
theta = posterior(lda_model)$topics,
vocab = colnames(dtm),
doc.length = rowSums(as.matrix(dtm)),
term.frequency = colSums(as.matrix(dtm))
)
serVis(lda_json)
for (i in 1:k) {
cat(sprintf("\nTopic %d:\n", i))
token_weights <- sort(topic_term_weights[i, ], decreasing = TRUE)
top_tokens <- head(token_weights, 10)
for (token in names(top_tokens)) {
cat(sprintf("%s (%.4f)\n", token, top_tokens[token]))
}
}
topic_word_counts <- colSums(posterior(lda_model)$topics)
# Create a data frame for visualization
topic_df <- data.frame(
Topic = paste("Topic", 1:k),
WordCount = topic_word_counts
)
# Plot the bar chart
library(ggplot2)
ggplot(topic_df, aes(x = Topic, y = WordCount, fill = Topic)) +
geom_bar(stat = "identity") +
labs(
title = "Number of Words in Each Topic",
x = "Topics",
y = "Word Count"
) +
theme_minimal()
for (i in 1:k) {
# Extract top words for the topic
token_weights <- sort(topic_term_weights[i, ], decreasing = TRUE)
top_tokens <- head(token_weights, 10)
# Create a data frame
topic_df <- data.frame(
Term = names(top_tokens),
Frequency = top_tokens
)
# Plot the bar chart
print(
ggplot(topic_df, aes(x = reorder(Term, -Frequency), y = Frequency, fill = Term)) +
geom_bar(stat = "identity") +
labs(
title = paste("Top Words in Topic", i),
x = "Words",
y = "Frequency"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
)
}
for (i in 1:k) {
# Extract top words for the topic
token_weights <- sort(topic_term_weights[i, ], decreasing = TRUE)
top_tokens <- head(token_weights, 10)
# Create a data frame
topic_df <- data.frame(
Term = names(top_tokens),
Frequency = top_tokens
)
# Plot the bar chart with frequency
print(
ggplot(topic_df, aes(x = reorder(Term, -Frequency), y = Frequency, fill = Term)) +
geom_bar(stat = "identity") +  # Use identity to plot actual frequencies
labs(
title = paste("Top Words in Topic", i),
x = "Words",
y = "Frequency"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
guides(fill = "none")  # Remove legend for clarity
)
}
# Loop through each topic and create a bar plot
for (i in 1:k) {
# Extract top words for the topic
token_weights <- sort(topic_term_weights[i, ], decreasing = TRUE)
top_tokens <- head(token_weights, 10)
# Create a data frame
topic_df <- data.frame(
Term = names(top_tokens),
Frequency = top_tokens
)
# Plot the bar chart with frequency
print(
ggplot(topic_df, aes(x = reorder(Term, -Frequency), y = Frequency, fill = Term)) +
geom_bar(stat = "identity") +  # Use identity to plot actual frequencies
labs(
title = paste("Top Words in Topic", i),
x = "Words",
y = "Frequency"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
guides(fill = "none")  # Remove legend for clarity
)
}
# Loop through each topic and create a bar plot with raw word counts
for (i in 1:k) {
# Extract top words and their raw counts for the topic
topic_word_counts <- raw_word_counts[i, ]
sorted_word_counts <- sort(topic_word_counts, decreasing = TRUE)
top_words <- head(sorted_word_counts, 10)
# Create a data frame for the top words
topic_df <- data.frame(
Term = names(top_words),
Frequency = top_words
)
# Plot the bar chart with raw word counts
print(
ggplot(topic_df, aes(x = reorder(Term, -Frequency), y = Frequency, fill = Term)) +
geom_bar(stat = "identity") +  # Plot actual frequencies
labs(
title = paste("Top Words in Topic", i),
x = "Words",
y = "Frequency (Count)"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
guides(fill = "none")  # Remove legend for clarity
)
}
# Get document-topic probabilities
doc_topic_probs <- posterior(lda_model)$topics
# Multiply document-topic probabilities with the original DTM to get raw word counts per topic
raw_word_counts <- t(doc_topic_probs) %*% as.matrix(dtm)
# Loop through each topic and create a bar plot with raw word counts
for (i in 1:k) {
# Extract top words and their raw counts for the topic
topic_word_counts <- raw_word_counts[i, ]
sorted_word_counts <- sort(topic_word_counts, decreasing = TRUE)
top_words <- head(sorted_word_counts, 10)
# Create a data frame for the top words
topic_df <- data.frame(
Term = names(top_words),
Frequency = top_words
)
# Plot the bar chart with raw word counts
print(
ggplot(topic_df, aes(x = reorder(Term, -Frequency), y = Frequency, fill = Term)) +
geom_bar(stat = "identity") +  # Plot actual frequencies
labs(
title = paste("Top Words in Topic", i),
x = "Words",
y = "Frequency (Count)"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
guides(fill = "none")  # Remove legend for clarity
)
}
for (i in 1:k) {
# Extract top words for the topic
token_weights <- sort(topic_term_weights[i, ], decreasing = TRUE)
top_tokens <- head(token_weights, 10)
# Create a data frame
topic_df <- data.frame(
Term = names(top_tokens),
Frequency = top_tokens
)
# Plot the bar chart
print(
ggplot(topic_df, aes(x = reorder(Term, -Frequency), y = Frequency, fill = Term)) +
geom_bar(stat = "identity") +
labs(
title = paste("Top Words in Topic", i),
x = "Words",
y = "Frequency"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
)
}
topic_word_counts <- colSums(posterior(lda_model)$topics)
topic_df <- data.frame(
Topic = paste("Topic", 1:k),
WordCount = topic_word_counts
)
ggplot(topic_df, aes(x = Topic, y = WordCount, fill = Topic)) +
geom_bar(stat = "identity") +
labs(
title = "Number of Words in Each Topic",
x = "Topics",
y = "Word Count"
) +
theme_minimal()
for (i in 1:k) {
# Extract top words for the topic
token_weights <- sort(topic_term_weights[i, ], decreasing = TRUE)
top_tokens <- head(token_weights, 10)
# Create a data frame
topic_df <- data.frame(
Term = names(top_tokens),
Frequency = top_tokens
)
# Plot the bar chart
print(
ggplot(topic_df, aes(x = reorder(Term, -Frequency), y = Frequency, fill = Term)) +
geom_bar(stat = "identity") +
labs(
title = paste("Top Words in Topic", i),
x = "Words",
y = "Frequency"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
)
}
print(dtm)
print(corpus)
print("Topics with token weights:")
for (i in 1:k) {
cat(sprintf("\nTopic %d:\n", i))
token_weights <- sort(topic_term_weights[i, ], decreasing = TRUE)
top_tokens <- head(token_weights, 10)
for (token in names(top_tokens)) {
cat(sprintf("%s (%.4f)\n", token, top_tokens[token]))
}
}
topic_word_counts <- colSums(posterior(lda_model)$topics)
topic_word_counts <- colSums(posterior(lda_model)$topics)
topic_df <- data.frame(
Topic = paste("Topic", 1:k),
WordCount = topic_word_counts
)
ggplot(topic_df, aes(x = Topic, y = WordCount, fill = Topic)) +
geom_bar(stat = "identity") +
labs(
title = "Number of Words in Each Topic",
x = "Topics",
y = "Word Count"
) +
theme_minimal()
for (i in 1:k) {
# Extract top words for the topic
token_weights <- sort(topic_term_weights[i, ], decreasing = TRUE)
top_tokens <- head(token_weights, 10)
# Create a data frame
topic_df <- data.frame(
Term = names(top_tokens),
Frequency = top_tokens
)
# Plot the bar chart
print(
ggplot(topic_df, aes(x = reorder(Term, -Frequency), y = Frequency, fill = Term)) +
geom_bar(stat = "identity") +
labs(
title = paste("Top Words in Topic", i),
x = "Words",
y = "Frequency"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
)
}
library(rvest)
library(tm)
library(tokenizers)
library(hunspell)
library(textstem)
library(textclean)
url <- "https://en.wikipedia.org/wiki/American_International_University-Bangladesh"
webpage <- read_html(url)
text_data <- html_text(html_nodes(webpage, "p"))
lower_case_data <- tolower(text_data)
print(lower_case_data)
no_punc_data <- gsub("[[:punct:]]", "", lower_case_data)
print(no_punc_data)
no_digit_data <- gsub("\\d+", "", no_punc_data)
no_space_data <- stripWhitespace(no_digit_data)
no_contraction_text <- replace_contraction(no_space_data)
emoji_pattern <- "[\U0001F600-\U0001F64F]"
emoticon_pattern <- "[:;]-?[)D(|P]"
text_no_emojis <- sapply(no_contraction_text, function(text) {
text <- gsub(emoji_pattern, "", text, perl = TRUE)
text <- gsub(emoticon_pattern, "", text, perl = TRUE)
return(text)
})
text_no_stopwords <- removeWords(text_no_emojis, stopwords("en"))
remove_misspelled_words <- function(text) {
words <- unlist(tokenize_words(text))
is_correct <- hunspell_check(words)
corrected_words <- words[is_correct]
return(paste(corrected_words, collapse = " "))
}
spell_checked_text <- sapply(text_no_stopwords, remove_misspelled_words)
tokens <- tokenize_words(spell_checked_text)
clean_tokens <- unlist(tokens)
clean_tokens <- lemmatize_words(clean_tokens)
clean_tokens <- clean_tokens[nchar(clean_tokens) > 1]
tokens_df <- data.frame(tokens = clean_tokens, stringsAsFactors = FALSE)
write.csv(tokens_df, "D://Educational/AIUB sem 9/MID/Data Science/Project2.csv", row.names = FALSE)
library(tm)
library(topicmodels)
library(ggplot2)
file_path <- "D://Educational/AIUB sem 9/MID/Data Science/Project2.csv"
data <- read.csv(file_path, stringsAsFactors = FALSE)
corpus <- Corpus(VectorSource(data$tokens))
print(corpus)
dtm <- DocumentTermMatrix(corpus)
print(dtm)
row_totals <- apply(dtm, 1, sum)
dtm <- dtm[row_totals > 0, ]
dtm_tfidf <- weightTfIdf(dtm)
print(dtm_tfidf)
k <- 5
lda_model <- LDA(dtm, k = k, control = list(seed = 123))
topic_term_weights <- posterior(lda_model)$terms
print("Topics with token weights:")
for (i in 1:k) {
cat(sprintf("\nTopic %d:\n", i))
token_weights <- sort(topic_term_weights[i, ], decreasing = TRUE)
top_tokens <- head(token_weights, 10)
for (token in names(top_tokens)) {
cat(sprintf("%s (%.4f)\n", token, top_tokens[token]))
}
}
topic_word_counts <- colSums(posterior(lda_model)$topics)
topic_df <- data.frame(
Topic = paste("Topic", 1:k),
WordCount = topic_word_counts
)
ggplot(topic_df, aes(x = Topic, y = WordCount, fill = Topic)) +
geom_bar(stat = "identity") +
labs(
title = "Number of Words in Each Topic",
x = "Topics",
y = "Word Count"
) +
theme_minimal()
for (i in 1:k) {
token_weights <- sort(topic_term_weights[i, ], decreasing = TRUE)
top_tokens <- head(token_weights, 10)
topic_df <- data.frame(
Term = names(top_tokens),
Frequency = top_tokens
)
print(
ggplot(topic_df, aes(x = reorder(Term, -Frequency), y = Frequency, fill = Term)) +
geom_bar(stat = "identity") +
labs(
title = paste("Top Words in Topic", i),
x = "Words",
y = "Frequency"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
)
}
